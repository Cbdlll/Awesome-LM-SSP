# C2. Data Reconstruction
- [2025/07] **[Model Inversion Attacks on Llama 3: Extracting PII from Large Language Models](https://arxiv.org/abs/2507.04478)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[System Prompt Extraction Attacks and Defenses in Large Language Models](https://arxiv.org/abs/2505.23817)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems through Benign Queries](https://arxiv.org/abs/2505.15420)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/05] **[ProxyPrompt: Securing System Prompts against Prompt Extraction Attacks](https://arxiv.org/abs/2505.11459)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[ReCIT: Reconstructing Full Private Data from Gradient in Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2504.20570)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[Diffusion-Driven Universal Model Inversion Attack for Face Recognition](https://arxiv.org/abs/2504.18015)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2025/03] **[Safeguarding LLM Embeddings in End-Cloud Collaboration via Entropy-Driven Perturbation](https://arxiv.org/abs/2503.12896)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/03] **[Prompt Inversion Attack against Collaborative Inference of Large Language Models](https://arxiv.org/abs/2503.09022)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![SP'25](https://img.shields.io/badge/SP'25-f1b800)
- [2025/03] **[Prompt Inference Attack on Distributed Large Language Model Inference Frameworks](https://arxiv.org/abs/2503.09291)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/02] **[Automating Prompt Leakage Attacks on Large Language Models Using Agentic Approach](https://arxiv.org/abs/2502.12630)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/02] **[ALGEN: Few-shot Inversion Attacks on Textual Embeddings using Alignment and Generation](https://arxiv.org/abs/2502.11308)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/02] **[MARAGE: Transferable Multi-Model Adversarial Attack for Retrieval-Augmented Generation Data Extraction](https://arxiv.org/abs/2502.04360)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/01] **[How Much Do Code Language Models Remember? An Investigation on Data Extraction Attacks before and after Fine-tuning](https://arxiv.org/abs/2501.17501)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/12] **[Safeguarding System Prompts for LLMs](https://arxiv.org/abs/2412.13426)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/11] **[RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented Generation Applications with Agent-based Attacks](https://arxiv.org/abs/2411.14110)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/11] **[Mitigating Privacy Risks in LLM Embeddings from Embedding Inversion](https://arxiv.org/abs/2411.05034)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/11] **[Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors](https://arxiv.org/abs/2411.01705)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/10] **[Stealing User Prompts from Mixture of Experts ](https://arxiv.org/abs/2410.22884)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Remote Timing Attacks on Efficient Language Model Inference](https://arxiv.org/abs/2410.17175)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Extracting Spatiotemporal Data from Gradients with Large Language Models](https://arxiv.org/abs/2410.16121)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Data Defenses Against Large Language Models](https://arxiv.org/abs/2410.13138)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/10] **[PII-Scope: A Benchmark for Training Data PII Leakage Assessment in LLMs](https://arxiv.org/abs/2410.06704)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2024/10] **[Towards a Theoretical Understanding of Memorization in Diffusion Models](https://arxiv.org/abs/2410.02467)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/09] **[Extracting Memorized Training Data via Decomposition](https://arxiv.org/abs/2409.12367)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[Prompt Obfuscation for Large Language Models](https://arxiv.org/abs/2409.11026)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/08] **[Transferable Embedding Inversion Attack: Uncovering Privacy Risks in Text Embeddings without Model Queries](https://aclanthology.org/2024.acl-long.230/)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/coffree0123/TEIA) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'24](https://img.shields.io/badge/ACL'24-f1b800)
- [2024/08] **[Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models](https://arxiv.org/abs/2408.02416)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?](https://arxiv.org/abs/2407.16607)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[Was it Slander? Towards Exact Inversion of Generative Language Models](https://arxiv.org/abs/2407.11059)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[Towards More Realistic Extraction Attacks: An Adversarial Perspective](https://arxiv.org/abs/2407.02596)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[PII-Compass: Guiding LLM training data extraction prompts towards the target PII via grounding](https://arxiv.org/abs/2407.02943)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'24](https://img.shields.io/badge/ACL'24-f1b800)
- [2024/06] **[Extracting Training Data from Unconditional Diffusion Models](https://arxiv.org/abs/2406.12752)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/06] **[Is Diffusion Model Safe? Severe Data Leakage via Gradient-Guided Diffusion Model](https://arxiv.org/abs/2406.09484)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/06] **[Raccoon: Prompt Extraction Benchmark of LLM-Integrated Applications](https://arxiv.org/abs/2406.06737)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'24_(Findings)](https://img.shields.io/badge/ACL'24_(Findings)-f1b800)
- [2024/05] **[Extracting Prompts by Inverting LLM Outputs](https://arxiv.org/abs/2405.15012)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[PLeak: Prompt Leaking Attacks against Large Language Model Applications](https://arxiv.org/abs/2405.06823)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![CCS'24](https://img.shields.io/badge/CCS'24-f1b800)
- [2024/05] **[Special Characters Attack: Toward Scalable Training Data Extraction From Large Language Models](https://arxiv.org/abs/2405.05990)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[Could It Be Generated? Towards Practical Analysis of Memorization in Text-To-Image Diffusion Models](https://arxiv.org/abs/2405.05846)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/04] **[Investigating the prompt leakage effect and black-box defenses for multi-turn LLM interactions](https://arxiv.org/abs/2404.16251)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Iteratively Prompting Multimodal LLMs to Reproduce Natural and AI-Generated Images](https://arxiv.org/abs/2404.13784)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/03] **[Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs](https://arxiv.org/abs/2403.04801)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[PRSA: Prompt Reverse Stealing Attacks against Large Language Models](https://arxiv.org/abs/2402.19200)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2402.17840)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/02] **[Pandora's White-Box: Increased Training Data Leakage in Open LLMs](https://arxiv.org/abs/2402.17012)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Large Language Models are Advanced Anonymizers](https://arxiv.org/abs/2402.13846)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Prompt Stealing Attacks Against Large Language Models](https://arxiv.org/abs/2402.12959)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Conversation Reconstruction Attack Against GPT Models ](https://arxiv.org/abs/2402.02987)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[Text Embedding Inversion Attacks on Multilingual Language Models](https://arxiv.org/abs/2401.12192)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://huggingface.co/yiyic) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/12] **[Diffence: Fencing Membership Privacy With Diffusion Models](https://arxiv.org/abs/2312.04692)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![NDSS'25](https://img.shields.io/badge/NDSS'25-f1b800)
- [2023/11] **[Scalable Extraction of Training Data from (Production) Language Models](https://arxiv.org/abs/2311.17035)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/09] **[Intriguing Properties of Data Attribution on Diffusion Models](https://openreview.net/forum?id=vKViCoKGcB)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Teach LLMs to Phish: Stealing Private Information from Language Models](https://openreview.net/forum?id=qo21ZlfNu6)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Language Model Inversion](https://arxiv.org/abs/2311.13647)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/07] **[Effective Prompt Extraction from Language Models](https://arxiv.org/abs/2307.06865v3)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/02] **[Prompt Stealing Attacks Against Text-to-Image Generation Models](https://arxiv.org/abs/2302.09923)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![USENIX_Security'24](https://img.shields.io/badge/USENIX_Security'24-f1b800)
- [2023/01] **[Extracting Training Data from Diffusion Models](https://arxiv.org/abs/2301.13188)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![USENIX_Security'23](https://img.shields.io/badge/USENIX_Security'23-f1b800)
- [2020/12] **[Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![USENIX_Security'21](https://img.shields.io/badge/USENIX_Security'21-f1b800)
