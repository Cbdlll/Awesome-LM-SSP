# LM-SSP

The resources related to the safety, security, and privacy (SSP) of large models (LM).
Here LM contains large language models (LLMs), large vision-language models (LVMs), diffusion models, and so on.

- This repo is in progress ðŸ”¥ (currently manually collected)

- Welcome to recommend resources to us (via Issue/Pull request/[Email](thu_crypto_ai@163.com)/...)!

- Tags:
![img](https://img.shields.io/badge/blog/paper/book-18a5ab)
![img](https://img.shields.io/badge/code-8B8989)
![img](https://img.shields.io/badge/defense-87b800)
![img](https://img.shields.io/badge/llm-c7688b)
![img](https://img.shields.io/badge/vlm-589cf4)
![img](https://img.shields.io/badge/diffusion-a99cf4)

# News
- ðŸ”¥ðŸ”¥ðŸ”¥ [2024-01-09] Add 8 papers from [CCS'23](https://www.sigsac.org/ccs/CCS2023/program.html)

# Books

- [2024/01] NIST Trustworthy and Responsible AI Reports [![img](https://img.shields.io/badge/book-18a5ab)](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2023.pdf) 

# Papers

## A. Safety
### A1. Jailbreak
- [2024/01] MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2401.02906) [![img](https://img.shields.io/badge/code-8B8989)](https://github.com/pipilurj/MLLM-protector) ![img](https://img.shields.io/badge/vlm-589cf4) ![img](https://img.shields.io/badge/defense-87b800) 
- [2023/12] Adversarial Attacks on GPT-4 via Simple Random Search [![img](https://img.shields.io/badge/paper-18a5ab)](https://www.andriushchenko.me/gpt4adv.pdf) ![img](https://img.shields.io/badge/llm-c7688b) 
- [2023/11] FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2311.05608) [![img](https://img.shields.io/badge/code-8B8989)](https://github.com/ThuCCSLab/FigStep) ![img](https://img.shields.io/badge/vlm-589cf4) 
- [2023/10] Adversarial Attacks on LLMs [![img](https://img.shields.io/badge/blog-18a5ab)](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/) ![img](https://img.shields.io/badge/llm-c7688b) 
- [2023/10] AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2310.04451) [![img](https://img.shields.io/badge/code-8B8989)](https://github.com/SheltonLiu-N/AutoDAN) ![img](https://img.shields.io/badge/llm-c7688b) 
- [2023/10] AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2310.04451) ![img](https://img.shields.io/badge/llm-c7688b) 
- [2023/10] Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2310.06987) [![img](https://img.shields.io/badge/code-8B8989)](https://github.com/Princeton-SysML/Jailbreak_LLM) ![img](https://img.shields.io/badge/llm-c7688b) 
- [2023/10] GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2309.10253)[![img](https://img.shields.io/badge/code-8B8989)](https://github.com/sherdencooper/GPTFuzz) ![img](https://img.shields.io/badge/llm-c7688b) 
- [2023/10] Jailbreaking Black Box Large Language Models in Twenty Queries [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2310.08419) [![img](https://img.shields.io/badge/code-8B8989)](https://github.com/patrickrchao/JailbreakingLLMs) ![img](https://img.shields.io/badge/llm-c7688b) 
- [2023/09] Open Sesame! Universal Black Box Jailbreaking of Large Language Models [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2309.01446) ![img](https://img.shields.io/badge/llm-c7688b) 
- [2023/07] MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2307.08715) ![img](https://img.shields.io/badge/llm-c7688b) ![img](https://img.shields.io/badge/NDSS'24-f1b800)
- [2023/07] Jailbroken: How Does LLM Safety Training Fail? [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2307.02483) ![img](https://img.shields.io/badge/llm-c7688b)
- [2023/07] Universal and Transferable Adversarial Attacks on Aligned Language Models [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2307.15043) [![img](https://img.shields.io/badge/code-8B8989)](https://github.com/llm-attacks/llm-attacks) ![img](https://img.shields.io/badge/llm-c7688b)

### A2. Safety Alignment
- [2024/01] A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/pdf/2401.01967.pdf) 
- [2023/12] Exploiting Novel GPT-4 APIs [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2312.14302) ![img](https://img.shields.io/badge/llm-c7688b) 
- [2023/10] Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To! [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2310.03693) [![img](https://img.shields.io/badge/code-8B8989)](https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety) ![img](https://img.shields.io/badge/llm-c7688b) 
- [2023/10] Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2310.02949v1) [![img](https://img.shields.io/badge/code-8B8989)](https://github.com/BeyonderXX/ShadowAlignment) ![img](https://img.shields.io/badge/llm-c7688b) 
- [2023/10] UltraFeedback: Boosting Language Models with High-quality Feedback [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2310.01377) [![img](https://img.shields.io/badge/code-8B8989)](https://github.com/OpenBMB/UltraFeedback) ![img](https://img.shields.io/badge/llm-c7688b) 

### A3. Toxicity
- [2023/08] You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2308.05596) ![img](https://img.shields.io/badge/llm-c7688b) ![img](https://img.shields.io/badge/S&P'24-f1b800)
- [2023/05] Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2305.13873) ![img](https://img.shields.io/badge/diffusion-a99cf4) ![img](https://img.shields.io/badge/CCS'23-f1b800)

### A4. Deepfake
- [2023/05] Evading Watermark based Detection of AI-Generated Content [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2305.03807) ![img](https://img.shields.io/badge/CCS'23-f1b800)
- [2023/03] MGTBench: Benchmarking Machine-Generated Text Detection [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2303.14822) [![img](https://img.shields.io/badge/code-8B8989)](https://github.com/xinleihe/MGTBench) ![img](https://img.shields.io/badge/llm-c7688b) 
- [2022/10] DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Generation Models [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2210.06998) ![img](https://img.shields.io/badge/diffusion-a99cf4) ![img](https://img.shields.io/badge/CCS'23-f1b800)

### A5. Agent
- [2023/11] Evil Geniuses: Delving into the Safety of LLM-based Agents [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/pdf/2311.11855.pdf) ![img](https://img.shields.io/badge/llm-c7688b)

## B. Security
### B1. Adversarial Attacks
- [2024/01] INSTRUCTTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/pdf/2312.01886.pdf) ![img](https://img.shields.io/badge/vlm-589cf4) 
- [2023/08] Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2308.07847) ![img](https://img.shields.io/badge/llm-c7688b) 
- [2023/06] PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2306.04528) ![img](https://img.shields.io/badge/llm-c7688b) 
- [2022/05] Diffusion Models for Adversarial Purification [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/pdf/2205.07460.pdf) ![img](https://img.shields.io/badge/diffusion-a99cf4) ![img](https://img.shields.io/badge/defense-87b800) ![img](https://img.shields.io/badge/ICML'22-f1b800) 

### B2. Code Generation
- [2023/02] Large Language Models for Code: Security Hardening and Adversarial Testing [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2302.05319) ![img](https://img.shields.io/badge/llm-c7688b) ![img](https://img.shields.io/badge/CCS'23-f1b800)
- [2022/11] Do Users Write More Insecure Code with AI Assistants? [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2211.03622) ![img](https://img.shields.io/badge/llm-c7688b) ![img](https://img.shields.io/badge/CCS'23-f1b800)

### B3. Backdoor/Poisoning
- [2023/05] Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2305.14710)  ![img](https://img.shields.io/badge/llm-c7688b)
- [2022/11] LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/pdf/2308.13904.pdf) [![img](https://img.shields.io/badge/code-8B8989)](https://github.com/meng-wenlong/LMSanitator) ![img](https://img.shields.io/badge/defense-87b800) ![img](https://img.shields.io/badge/NDSS'24-f1b800)

## C. Privacy
### C1. Data Reconstruction
- [2023/11] Scalable Extraction of Training Data from (Production) Language Models [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2311.17035) ![img](https://img.shields.io/badge/llm-c7688b)
- [2023/01] Extracting Training Data from Diffusion Models [![img](https://img.shields.io/badge/paper-18a5ab)](https://www.usenix.org/system/files/usenixsecurity23-carlini.pdf) ![img](https://img.shields.io/badge/diffusion-a99cf4) ![img](https://img.shields.io/badge/Security'23-f1b800)
- [2020/12] Extracting Training Data from Large Language Models [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2012.07805) ![img](https://img.shields.io/badge/llm-c7688b) ![img](https://img.shields.io/badge/Security'21-f1b800)

### C2. Membership Inference
- [2023/09] DP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2309.06746) ![img](https://img.shields.io/badge/defense-87b800) ![img](https://img.shields.io/badge/CCS'23-f1b800)


### C3. Property Inference
- [2023/10] Beyond Memorization: Violating Privacy Via Inference with Large Language Models [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2310.07298) ![img](https://img.shields.io/badge/llm-c7688b)

### C4. Model Extraction
- [2023/03] Stealing the Decoding Algorithms of Language Models [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2303.04729) ![img](https://img.shields.io/badge/llm-c7688b) ![img](https://img.shields.io/badge/CCS'23-f1b800)

### C5. Unlearning
- [2023/10] Unlearn What You Want to Forget: Efficient Unlearning for LLMs [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2310.20150) ![img](https://img.shields.io/badge/llm-c7688b) ![img](https://img.shields.io/badge/defense-87b800) 
- [2023/10] Who's Harry Potter? Approximate Unlearning in LLMs [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2310.02238?s=08) ![img](https://img.shields.io/badge/llm-c7688b) ![img](https://img.shields.io/badge/defense-87b800) 
- [2023/03] Erasing Concepts from Diffusion Models [![img](https://img.shields.io/badge/paper-18a5ab)](https://arxiv.org/abs/2303.07345) ![img](https://img.shields.io/badge/diffusion-a99cf4) ![img](https://img.shields.io/badge/defense-87b800) 

### C6. Copyright
- [2024/01] Generative AI Has a Visual Plagiarism Problem [![img](https://img.shields.io/badge/blog-18a5ab)](https://spectrum.ieee.org/midjourney-copyright)
- [2023/11] Protecting Intellectual Property of Large Language Model-Based Code Generation APIs via Watermarks [![img](https://img.shields.io/badge/paper-18a5ab)](https://dl.acm.org/doi/abs/10.1145/3576915.3623120) ![img](https://img.shields.io/badge/llm-c7688b) ![img](https://img.shields.io/badge/defense-87b800)  ![img](https://img.shields.io/badge/CCS'23-f1b800)
