<img src="figure/title.png" alt="image" width="1000" height="auto" class="center">

# Introduction 

The resources related to the trustworthiness of large models (LMs) across multiple dimensions (e.g., safety, security, and privacy),                  with a special focus on multi-modal LMs (e.g., vision-language models and diffusion models). 

- This repo is in progress :seedling: (currently manually collected).
- Welcome to recommend resources to us (via <a href="https://github.com/ThuCCSLab/lm-ssp/issues">                 <img src="https://icons.iconarchive.com/icons/github/octicons/128/issue-opened-16-icon.png" width="15" height="15"></a>                 Issues / <a href="https://github.com/ThuCCSLab/lm-ssp/pulls"><img src="https://icons.iconarchive.com/icons/iconoir-team/iconoir/128/git-pull-request-icon.png"                  width="17" height="17"></a> Pull requests / <a href="mailto:thu_crypto_ai@163.com"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/solid/envelope-open.svg"                 width="15" height="15"></a> Email / ...)!
- Badges: 

    - Model: ![img](https://img.shields.io/badge/llm-589cf4) ![img](https://img.shields.io/badge/vlm-c7688b)  ![img](https://img.shields.io/badge/diffusion-a99cf4) 

    - Comment: ![img](https://img.shields.io/badge/Benchmark-87b800) ![img](https://img.shields.io/badge/New_dataset-87b800) ![img](https://img.shields.io/badge/Agent-87b800)                 ![img](https://img.shields.io/badge/CodeGen-87b800) ![img](https://img.shields.io/badge/Defense-87b800) ![img](https://img.shields.io/badge/RAG-87b800) ![img](https://img.shields.io/badge/Chinese-87b800) 

   - Venue (Continuous update): ![img](https://img.shields.io/badge/conference-f1b800) or ![img](https://img.shields.io/badge/blog-f1b800)

 

# Book

-  [2024/01] **[NIST Trustworthy and Responsible AI Reports](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2023.pdf)**

# Survey

-  [2023/05] **[ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review](https://arxiv.org/abs/2305.03123)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Privacy Issues in Large Language Models: A Survey](https://arxiv.org/abs/2312.06717)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/01] **[TrustLLM: Trustworthiness in Large Language Models](https://arxiv.org/abs/2401.05561)** <a href="https://github.com/HowieHwong/TrustLLM"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly](https://arxiv.org/abs/2312.02003)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks](https://arxiv.org/abs/2310.10844)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/08] **[Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment](https://arxiv.org/abs/2308.05374)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[A Comprehensive Overview of Large Language Models](https://arxiv.org/abs/2307.06435)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/06] **[DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)** <a href="https://decodingtrust.github.io/"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800) ![img](https://img.shields.io/badge/Best_paper-ff0000)
-  [2023/04] **[Safety Assessment of Chinese Large Language Models](https://arxiv.org/abs/2304.10436)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/03] **[A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2022/11] **[Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110)** ![img](https://img.shields.io/badge/TMLR'23-f1b800)
-  [2022/06] **[Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models](https://arxiv.org/abs/2206.04615)**
-  [2021/11] **[Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models](https://arxiv.org/abs/2111.02840)** ![img](https://img.shields.io/badge/NeurIPS'21-f1b800)

# Paper

<img src="figure/map.png" alt="image" width="1000" height="auto" class="center">



## A. Security


### A1. Adversarial Examples

-  [2024/01] **[INSTRUCTTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models](https://arxiv.org/abs/2312.01886)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/12] **[Hijacking Context in Large Multi-modal Models](https://arxiv.org/abs/2312.07553)**
-  [2023/12] **[Causality Analysis for Evaluating the Security of Large Language Models](https://arxiv.org/abs/2312.07876)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](https://arxiv.org/abs/2311.16101)** <a href="https://github.com/UCSC-VLAA/vllm-safety-benchmark"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/Benchmark-87b800)
-  [2023/11] **[DiffAttack: Evasion Attacks Against Diffusion-Based Adversarial Purification](https://arxiv.org/abs/2311.16124)** <a href="https://github.com/kangmintong/DiffAttack"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/11] **[Can Protective Perturbation Safeguard Personal Data from Being Exploited by Stable Diffusion?](https://arxiv.org/abs/2312.00084)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/11] **[Unveiling Safety Vulnerabilities of Large Language Models](https://arxiv.org/abs/2311.04124)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Misusing Tools in Large Language Models With Visual Adversarial Examples](https://arxiv.org/abs/2310.03185)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/10] **[An LLM can Fool Itself: A Prompt-Based Adversarial Attack](https://arxiv.org/abs/2310.13345)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[How Robust is Google's Bard to Adversarial Image Attacks?](https://arxiv.org/abs/2309.11751)** <a href="https://github.com/thu-ml/Attack-Bard"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/09] **[Image Hijacks: Adversarial Images Can Control Generative Models at Runtime](https://arxiv.org/abs/2309.00236)** <a href="https://github.com/euanong/image-hijacks"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/08] **[Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings](https://arxiv.org/abs/2308.11804)** <a href="https://github.com/ebagdasa/adversarial_illusions"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/08] **[On the Adversarial Robustness of Multi-Modal Foundation Models](https://arxiv.org/abs/2308.10741)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICCV'23_(AROW_Workshop)-f1b800)
-  [2023/08] **[Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models](https://arxiv.org/abs/2308.07847)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[Jailbreak in Pieces: Compositional Adversarial Attacks on Multi-Modal Language Models](https://arxiv.org/abs/2307.14539)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/07] **[Certified Robustness for Large Language Models with Self-Denoising](https://arxiv.org/abs/2307.07171)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/06] **[Adversarial Examples in the Age of ChatGPT](http://spylab.ai/blog/chatbot-adversarial-examples/)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/Blog-f1b800)
-  [2023/06] **[Visual Adversarial Examples Jailbreak Large Language Models](https://arxiv.org/abs/2306.13213)** <a href="https://github.com/Unispac/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/06] **[Stable Diffusion is Unstable](https://arxiv.org/abs/2306.02583)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/06] **[Unlearnable Examples for Diffusion Models: Protect Data from Unauthorized Exploitation](https://arxiv.org/abs/2306.01902)** <a href="https://github.com/ZhengyueZhao/EUDP"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2023/06] **[PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts](https://arxiv.org/abs/2306.04528)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Benchmark-87b800)
-  [2023/06] **[Are aligned neural networks adversarially aligned?](https://arxiv.org/abs/2306.15447)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/05] **[On evaluating adversarial robustness of large vision-language models](https://arxiv.org/abs/2305.16934)** <a href="https://github.com/yunqing-me/AttackVLM"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/05] **[Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility](https://arxiv.org/abs/2305.10235)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/03] **[Anti-DreamBooth: Protecting Users from Personalized Text-to-Image Synthesis](https://arxiv.org/abs/2303.15433)** <a href="https://github.com/VinAIResearch/Anti-DreamBooth"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICCV'23-f1b800)
-  [2023/02] **[Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples](https://arxiv.org/abs/2302.04578)** <a href="https://github.com/mist-project/mist"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICML'23-f1b800)
-  [2023/02] **[Raising the Cost of Malicious AI-Powered Image Editing](https://arxiv.org/abs/2302.06588)** <a href="https://github.com/MadryLab/photoguard"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/ICML'23-f1b800)
-  [2023/02] **[On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective](https://arxiv.org/abs/2302.12095)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/02] **[Large Language Models Can Be Easily Distracted by Irrelevant Context](https://arxiv.org/abs/2302.00093)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICML'23-f1b800)
-  [2023/01] **[On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex](https://arxiv.org/abs/2301.12868)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/CodeGen-87b800) ![img](https://img.shields.io/badge/EACL'23-f1b800)
-  [2022/12] **[Understanding Zero-shot Adversarial Robustness for Large-Scale Model](https://arxiv.org/abs/2212.07016)** <a href="https://github.com/cvlab-columbia/ZSRobust4FoundationModel"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/ICLR'23-f1b800)

### A2. Code Generation Security

-  [2024/01] **[DebugBench: Evaluating Debugging Capability of Large Language Models](https://arxiv.org/abs/2401.04621)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Chain of Code: Reasoning with a Language Model-Augmented Code Emulator](https://arxiv.org/abs/2312.04474)**
-  [2023/12] **[Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models](https://arxiv.org/abs/2312.04724)** <a href="PurpleLlama/CybersecurityBenchmarks at main · facebookresearch/PurpleLlama (github.com)"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/CodeGen-87b800)
-  [2023/05] **[Uncovering and Quantifying Social Biases in Code Generation](https://arxiv.org/abs/2305.15377)**
-  [2023/02] **[Large Language Models for Code: Security Hardening and Adversarial Testing](https://arxiv.org/abs/2302.05319)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/CCS'23-f1b800) ![img](https://img.shields.io/badge/Best_paper-ff0000)
-  [2022/11] **[Do Users Write More Insecure Code with AI Assistants?](https://arxiv.org/abs/2211.03622)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/CCS'23-f1b800)

### A3. Poisoning 

-  [2024/01] **[Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://arxiv.org/abs/2401.05566)** <a href="https://github.com/anthropics/sleeper-agents-paper"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Unleashing Cheapfakes through Trojan Plugins of Large Language Models](https://arxiv.org/abs/2312.00374)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](https://arxiv.org/abs/2312.00027)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers' Coding Practices with Insecure Suggestions from Poisoned AI Models](https://arxiv.org/abs/2312.06227)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Universal Jailbreak Backdoors from Poisoned Human Feedback](https://arxiv.org/abs/2311.14455)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations](https://arxiv.org/abs/2311.09763)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/10] **[Composite Backdoor Attacks Against Large Language Models](https://arxiv.org/abs/2310.07676)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models](https://arxiv.org/abs/2310.13828)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Prompt Injection Attacks and Defenses in LLM-Integrated Applications](https://arxiv.org/abs/2310.12815)** <a href="liu00222/Open-Prompt-Injection: Prompt injection attacks and defenses in LLM-integrated applications (github.com)"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/10] **[Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers](https://arxiv.org/abs/2310.18603)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/10] **[PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models](https://arxiv.org/abs/2310.12439)** <a href="grasses/PoisonPrompt: Code for paper: PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models, IEEE ICASSP 2024 (github.com)"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICASSP'24-f1b800)
-  [2023/08] **[The Poison of Alignment](https://arxiv.org/abs/2308.13449)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/08] **[LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors](https://arxiv.org/abs/2308.13904)** <a href="https://github.com/meng-wenlong/LMSanitator"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NDSS'24-f1b800)
-  [2023/07] **[Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection](https://arxiv.org/abs/2307.16888)** <a href="https://github.com/wegodev2/virtual-prompt-injection"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/06] **[On the Exploitability of Instruction Tuning](https://arxiv.org/abs/2306.17194)** <a href="https://github.com/azshue/AutoPoison"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/06] **[Prompt Injection attack against LLM-integrated Applications](https://arxiv.org/abs/2306.05499)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Poisoning Language Models During Instruction Tuning](https://arxiv.org/abs/2305.00944)** <a href="https://github.com/AlexWan0/Poisoning-Instruction-Tuned-Models"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICML'23-f1b800)
-  [2023/05] **[Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](https://arxiv.org/abs/2305.14710)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/02] **[Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/abs/2302.12173v2)** ![img](https://img.shields.io/badge/LLM-589cf4)

## B. Safety


### B1. Deepfake

-  [2023/10] **[Harnessing the Power of ChatGPT in Fake News: An In-Depth Exploration in Generation, Detection and Explanation](https://arxiv.org/abs/2310.05046)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Evading Watermark based Detection of AI-Generated Content](https://arxiv.org/abs/2305.03807)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/CCS'23-f1b800)
-  [2023/05] **[On the Risk of Misinformation Pollution with Large Language Models](https://arxiv.org/abs/2305.13661)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/04] **[Synthetic Lies: Understanding AI-Generated Misinformation and Evaluating Algorithmic and Human Solutions](https://doi.org/10.1145/3544548.3581318)** ![img](https://img.shields.io/badge/CHI'23-f1b800)
-  [2023/03] **[Can AI-Generated Text be Reliably Detected?](https://arxiv.org/abs/2303.11156)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/03] **[MGTBench: Benchmarking Machine-Generated Text Detection](https://arxiv.org/abs/2303.14822)** <a href="https://github.com/xinleihe/MGTBench"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2022/12] **[CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data Limitation With Contrastive Learning](https://arxiv.org/abs/2212.10341)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2022/10] **[DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Generation Models](https://arxiv.org/abs/2210.06998)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/CCS'23-f1b800)

### B2. Ethics

-  [2023/12] **[Disentangling Perceptions of Offensiveness: Cultural and Moral Correlates](https://arxiv.org/abs/2312.06861)**
-  [2023/10] **[Unpacking the Ethical Value Alignment in Big Models](https://arxiv.org/abs/2310.17551)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning](https://arxiv.org/abs/2310.11053)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Benchmark-87b800)
-  [2023/05] **[From Text to MITRE Techniques: Exploring the Malicious Use of Large Language Models for Generating Cyber Attack Payloads](https://arxiv.org/abs/2305.15336)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/01] **[Exploring AI Ethics of ChatGPT: A Diagnostic Analysis](https://arxiv.org/abs/2301.12867)** ![img](https://img.shields.io/badge/LLM-589cf4)

### B3. Fairness

-  [2023/12] **[GPTBIAS: A Comprehensive Framework for Evaluating Bias in Large Language Models](https://arxiv.org/abs/2312.06315)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity](https://arxiv.org/abs/2311.18580)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models](https://arxiv.org/abs/2311.09428)**
-  [2023/11] **[ROBBIE: Robust Bias Evaluation of Large Generative Language Models](https://arxiv.org/abs/2311.18140)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23-f1b800)
-  [2023/10] **[Investigating the Fairness of Large Language Models for Predictions on Tabular Data](https://arxiv.org/abs/2310.14607)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Kelly is a Warm Person, Joseph is a Role Model: Gender Biases in LLM-Generated Reference Letters](https://arxiv.org/abs/2310.09219)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23 (Findings)-f1b800)
-  [2023/10] **[Im not Racist but...: Discovering Bias in the Internal Knowledge of Large Language Models](https://arxiv.org/abs/2310.08780)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Bias and Fairness in Large Language Models: A Survey](https://arxiv.org/abs/2309.00770)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Bias and Fairness in Chatbots: An Overview](https://arxiv.org/abs/2309.08836)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[People's Perceptions Toward Bias and Related Concepts in Large Language Models: A Systematic Review](https://arxiv.org/abs/2309.14504)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/08] **[FairBench: A Four-Stage Automatic Framework for Detecting Stereotypes and Biases in Large Language Models](https://arxiv.org/abs/2308.10397)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/08] **[Gender bias and stereotypes in Large Language Models](https://arxiv.org/abs/2308.14921)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/CI'23-f1b800)
-  [2023/07] **[Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models](https://arxiv.org/abs/2307.00101)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/06] **[WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models](https://arxiv.org/abs/2306.15087)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ACL'23-f1b800)
-  [2023/06] **[Knowledge of cultural moral norms in large language models](https://arxiv.org/abs/2306.01857)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ACL'23-f1b800)
-  [2023/05] **[Large Language Models are not Fair Evaluators](https://arxiv.org/abs/2305.17926)** <a href="https://github.com/i-Eval/FairEval"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation](https://arxiv.org/abs/2305.07609)** <a href="https://github.com/jizhi-zhang/FaiRLLM"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Recsys'23-f1b800)
-  [2023/05] **[BiasAsker: Measuring the Bias in Conversational AI System](https://arxiv.org/abs/2305.12434)** ![img](https://img.shields.io/badge/FSE'23-f1b800)
-  [2022/05] **[Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts](https://aclanthology.org/2022.acl-long.72/)** <a href="https://github.com/Irenehere/Auto-Debias"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800) ![img](https://img.shields.io/badge/ACL'22-f1b800)
-  [2022/03] **[Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal](https://arxiv.org/abs/2203.12574)** ![img](https://img.shields.io/badge/ACL'22_(Findings)-f1b800)
-  [2021/04] **[Mitigating Political Bias in Language Models Through Reinforced Calibration](https://arxiv.org/abs/2104.14795)** ![img](https://img.shields.io/badge/AAAI'21-f1b800)
-  [2021/02] **[Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models](https://arxiv.org/abs/2102.04130)** ![img](https://img.shields.io/badge/NeurIPS'21-f1b800)
-  [2021/01] **[Persistent Anti-Muslim Bias in Large Language Models](https://arxiv.org/abs/2101.05783)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/AIES'21-f1b800)

### B4. Hallucination

-  [2024/01] **[Seven Failure Points When Engineering a Retrieval Augmented Generation System](https://arxiv.org/abs/2401.05856)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/RAG-87b800)
-  [2023/12] **[The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation](https://arxiv.org/abs/2312.09085)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Improving Factual Error Correction by Learning to Inject Factual Errors](https://arxiv.org/abs/2312.07049)**
-  [2023/12] **[RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback](https://arxiv.org/abs/2312.00849)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[DelucionQA: Detecting Hallucinations in Domain-specific Question Answering](https://arxiv.org/abs/2312.05200)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/11] **[Calibrated Language Models Must Hallucinate](https://arxiv.org/abs/2311.14648)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models](https://arxiv.org/abs/2311.09210)**
-  [2023/11] **[Fine-tuning Language Models for Factuality](https://arxiv.org/abs/2311.08401)**
-  [2023/11] **[UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation](https://arxiv.org/abs/2311.15296)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Chinese-87b800)
-  [2023/11] **[Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination](https://arxiv.org/abs/2311.15548)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[When Large Language Models contradict humans? Large Language Models' Sycophantic Behaviour](https://arxiv.org/abs/2311.09410)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/abs/2311.05232)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting](https://arxiv.org/abs/2311.13314)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus](https://arxiv.org/abs/2311.13230)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23-f1b800)
-  [2023/11] **[Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/abs/2311.03287)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/10] **[Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity](https://arxiv.org/abs/2310.07521)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models](https://arxiv.org/abs/2310.05253)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/09] **[Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models](https://arxiv.org/abs/2309.01219)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/08] **[Simple synthetic data reduces sycophancy in large language models](https://arxiv.org/abs/2308.03958)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models](https://arxiv.org/abs/2307.01379)** <a href="https://github.com/jinhaoduan/shifting-attention-to-relevance"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation](https://arxiv.org/abs/2307.03987)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/06] **[Explore, Establish, Exploit: Red Teaming Language Models from Scratch](https://arxiv.org/abs/2306.09442)**
-  [2023/06] **[Inference-Time Intervention: Eliciting Truthful Answers from a Language Model](https://arxiv.org/abs/2306.03341)**
-  [2023/05] **[Improving Factuality and Reasoning in Language Models through Multiagent Debate](https://arxiv.org/abs/2305.14325)**
-  [2023/05] **[Fact-Checking Complex Claims with Program-Guided Reasoning](https://arxiv.org/abs/2305.12744)** ![img](https://img.shields.io/badge/ACL'23-f1b800)
-  [2023/05] **[Trusting Your Evidence: Hallucinate Less with Context-aware Decoding](https://arxiv.org/abs/2305.14739)**
-  [2023/05] **[HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2305.11747)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23-f1b800)
-  [2023/05] **[Sources of Hallucination by Large Language Models on Inference Tasks](https://arxiv.org/abs/2305.14552)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/05] **[Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models](https://arxiv.org/abs/2305.13712)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment](https://arxiv.org/abs/2305.13669)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/04] **[In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT](https://arxiv.org/abs/2304.08979)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/03] **[SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models](https://arxiv.org/abs/2303.08896)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23-f1b800)
-  [2023/02] **[Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback](https://arxiv.org/abs/2302.12813)** <a href="https://github.com/pengbaolin/LLM-Augmenter"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/02] **[A Categorical Archive of ChatGPT Failures](https://arxiv.org/abs/2302.03494)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2022/02] **[Survey of Hallucination in Natural Language Generation](https://arxiv.org/abs/2202.03629)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2022/02] **[Locating and Editing Factual Associations in GPT](https://arxiv.org/abs/2202.05262)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NeurIPS'22-f1b800)

### B5. Jailbreak

-  [2024/01] **[How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLM](https://www.yi-zeng.com/wp-content/uploads/2024/01/view.pdf)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2024/01] **[MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance](https://arxiv.org/abs/2401.02906)** <a href="https://github.com/pipilurj/MLLM-protector"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/12] **[A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection](https://arxiv.org/abs/2312.10766)** ![img](https://img.shields.io/badge/VLM-c7688b) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/12] **[Adversarial Attacks on GPT-4 via Simple Random Search](https://www.andriushchenko.me/gpt4adv.pdf)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs](https://arxiv.org/abs/2312.04782)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/CodeGen-87b800)
-  [2023/11] **[FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts](https://arxiv.org/abs/2311.05608)** <a href="https://github.com/ThuCCSLab/FigStep"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/11] **[Evil Geniuses: Delving into the Safety of LLM-based Agents](https://arxiv.org/abs/2311.11855)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Agent-87b800)
-  [2023/11] **[Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild](https://arxiv.org/abs/2311.06237)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations](https://arxiv.org/abs/2310.06387)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Adversarial Attacks on LLMs](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Blog-f1b800)
-  [2023/10] **[AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models](https://arxiv.org/abs/2310.04451)** <a href="https://github.com/SheltonLiu-N/AutoDAN"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2310.15140)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation](https://arxiv.org/abs/2310.06987)** <a href="https://github.com/Princeton-SysML/Jailbreak_LLM"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419)** <a href="https://github.com/patrickrchao/JailbreakingLLMs"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese](https://arxiv.org/abs/2310.05818)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Chinese-87b800)
-  [2023/10] **[Low-Resource Languages Jailbreak GPT-4](https://arxiv.org/abs/2310.02446)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2310.03684)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/09] **[GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts](https://arxiv.org/abs/2309.10253)** <a href="https://github.com/sherdencooper/GPTFuzz"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Open Sesame! Universal Black Box Jailbreaking of Large Language Models](https://arxiv.org/abs/2309.01446)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models](https://arxiv.org/abs/2309.05274)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/08] **[Detecting Language Model Attacks with Perplexity](https://arxiv.org/abs/2308.14132)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/08] **[“Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models](https://arxiv.org/abs/2308.03825)** <a href="https://github.com/verazuo/jailbreak_llms"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/08] **[GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher](https://arxiv.org/abs/2308.06463)** <a href="https://github.com/RobustNLP/CipherChat"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/08] **[XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models](https://arxiv.org/abs/2308.01263)** <a href="paul-rottger/exaggerated-safety: Röttger et al. (2023): "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models" (github.com)"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800)
-  [2023/07] **[MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots](https://arxiv.org/abs/2307.08715)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NDSS'24-f1b800)
-  [2023/07] **[Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/07] **[Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)** <a href="https://github.com/llm-attacks/llm-attacks"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models](https://arxiv.org/abs/2307.08487)** <a href="qiuhuachuan/latent-jailbreak (github.com)"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Tricking LLMs into Disobedience: Understanding, Analyzing, and Preventing Jailbreaks](https://arxiv.org/abs/2305.14965)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study](https://arxiv.org/abs/2305.13860)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/04] **[Multi-step Jailbreaking Privacy Attacks on ChatGPT](https://arxiv.org/abs/2304.05197)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)

### B6. Safety Alignment

-  [2024/01] **[Agent Alignment in Evolving Social Norms](https://arxiv.org/abs/2401.04620)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Agent-87b800)
-  [2023/12] **[Exploiting Novel GPT-4 APIs](https://arxiv.org/abs/2312.14302)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Alignment for Honesty](https://arxiv.org/abs/2312.07000)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack](https://arxiv.org/abs/2312.06924)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Removing RLHF Protections in GPT-4 via Fine-Tuning](https://arxiv.org/abs/2311.05553)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!](https://arxiv.org/abs/2310.03693)** <a href="https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models](https://arxiv.org/abs/2310.02949v1)** <a href="https://github.com/BeyonderXX/ShadowAlignment"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[AI Alignment: A Comprehensive Survey](https://arxiv.org/abs/2310.19852)** <a href="https://github.com/PKU-Alignment/AlignmentSurvey?tab=readme-ov-file"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions](https://arxiv.org/abs/2309.07875)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM](https://arxiv.org/abs/2309.14348)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/08] **[You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content](https://arxiv.org/abs/2308.05596)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/S&P'24-f1b800)
-  [2023/07] **[CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility](https://arxiv.org/abs/2307.09705)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Chinese-87b800)
-  [2023/07] **[BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset](https://arxiv.org/abs/2307.04657)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision](https://arxiv.org/abs/2305.03047)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/04] **[Fundamental Limitations of Alignment in Large Language Models](https://arxiv.org/abs/2304.11082)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/04] **[RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment](https://arxiv.org/abs/2304.06767)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2022/10] **[Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values](https://arxiv.org/abs/2210.07652)**

### B7. Toxicity

-  [2024/01] **[A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity](https://arxiv.org/abs/2401.01967)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[GTA: Gated Toxicity Avoidance for LM Performance Preservation](https://arxiv.org/abs/2312.06122)** ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/12] **[Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models](https://arxiv.org/abs/2312.08303)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/12] **[Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations](https://arxiv.org/abs/2312.06674)** <a href="https://github.com/facebookresearch/PurpleLlama/tree/main/Llama-Guard"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Unveiling the Implicit Toxicity in Large Language Models](https://arxiv.org/abs/2311.17391)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23-f1b800)
-  [2023/10] **[On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts](https://arxiv.org/abs/2310.16613)** ![img](https://img.shields.io/badge/VLM-c7688b)
-  [2023/10] **[Safe RLHF: Safe Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2310.12773)**
-  [2023/10] **[All Languages Matter: On the Multilingual Safety of Large Language Models](https://arxiv.org/abs/2310.00905)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/08] **[Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs](https://arxiv.org/abs/2308.13387)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models](https://arxiv.org/abs/2305.13873)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/CCS'23-f1b800)
-  [2023/05] **[Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection](https://arxiv.org/abs/2305.13276)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/04] **[Toxicity in ChatGPT: Analyzing Persona-assigned Language Models](https://arxiv.org/abs/2304.05335)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/02] **[Adding Instructions during Pretraining: Effective Way of Controlling Toxicity in Language Models](https://arxiv.org/abs/2302.07388)** ![img](https://img.shields.io/badge/EACL'23-f1b800)
-  [2023/02] **[Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech](https://arxiv.org/abs/2302.07736)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/WWW'23_(Companion_Volume)-f1b800)
-  [2022/12] **[On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning](https://arxiv.org/abs/2212.08061)** ![img](https://img.shields.io/badge/ACL'23-f1b800)
-  [2022/12] **[Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)**
-  [2022/10] **[Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization](https://arxiv.org/abs/2210.04492)** ![img](https://img.shields.io/badge/ICLR'23-f1b800)
-  [2022/05] **[Toxicity Detection with Generative Prompt-based Inference](https://arxiv.org/abs/2205.12390)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2022/04] **[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862)**
-  [2022/03] **[ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509)** ![img](https://img.shields.io/badge/ACL'22-f1b800)
-  [2020/09] **[RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)** <a href="https://github.com/allenai/real-toxicity-prompts?tab=readme-ov-file"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'20_(Findings)-f1b800)

## C. Privacy


### C1. Copyright

-  [2024/01] **[Generative AI Has a Visual Plagiarism Problem](https://spectrum.ieee.org/midjourney-copyright)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/Blog-f1b800)
-  [2023/12] **[Mark My Words: Analyzing and Evaluating Language Model Watermarks](https://arxiv.org/abs/2312.00273)** <a href="https://github.com/wagner-group/MarkMyWords"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[A Robust Semantics-based Watermark for Large Language Model against Paraphrasing](https://arxiv.org/abs/2311.08721)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Protecting Intellectual Property of Large Language Model-Based Code Generation APIs via Watermarks](https://dl.acm.org/doi/abs/10.1145/3576915.3623120)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/CodeGen-87b800) ![img](https://img.shields.io/badge/CCS'23-f1b800)
-  [2023/08] **[PromptCARE: Prompt Copyright Protection by Watermark Injection and Verification](https://arxiv.org/abs/2308.02816)** <a href="https://github.com/grasses/PromptCARE"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/S&P'24-f1b800)
-  [2023/06] **[Generative Watermarking Against Unauthorized Subject-Driven Image Synthesis](https://arxiv.org/abs/2306.07754)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2023/05] **[Watermarking Diffusion Model](https://arxiv.org/abs/2305.12502)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2023/05] **[Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust](https://arxiv.org/abs/2305.20030)** <a href="https://github.com/YuxinWenRick/tree-ring-watermark"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)
-  [2023/02] **[Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models](https://arxiv.org/abs/2302.04222)** <a href="https://glaze.cs.uchicago.edu/"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Security'23-f1b800) ![img](https://img.shields.io/badge/Best_paper-ff0000)
-  [2023/01] **[A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)** <a href="github.com/jwkirchenbauer/lm-watermarking"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICML'23-f1b800) ![img](https://img.shields.io/badge/Best_paper-ff0000)

### C2. Data Reconstruction

-  [2023/11] **[Language Model Inversion](https://arxiv.org/abs/2311.13647)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/11] **[Scalable Extraction of Training Data from (Production) Language Models](https://arxiv.org/abs/2311.17035)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks](https://arxiv.org/abs/2309.17410)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/01] **[Extracting Training Data from Diffusion Models](https://arxiv.org/abs/2301.13188)** ![img](https://img.shields.io/badge/Diffusion-a99cf4) ![img](https://img.shields.io/badge/Security'23-f1b800)
-  [2020/12] **[Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Security'21-f1b800)

### C3. Differential Privacy

-  [2023/10] **[Locally Differentially Private Document Generation Using Zero Shot Prompting](https://arxiv.org/abs/2310.16111)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
-  [2023/09] **[DP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass](https://arxiv.org/abs/2309.06746)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/Defense-87b800) ![img](https://img.shields.io/badge/CCS'23-f1b800)
-  [2023/05] **[Privacy-Preserving Prompt Tuning for Large Language Model Services](https://arxiv.org/abs/2305.06212)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/05] **[Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models](https://arxiv.org/abs/2305.05973)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2022/10] **[EW-Tune: A Framework for Privately Fine-Tuning Large Language Models with Differential Privacy](https://arxiv.org/abs/2210.15042)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICDM'22_(Workshops)-f1b800)

### C4. Extraction 

-  [2023/11] **[Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition](https://arxiv.org/abs/2311.16119)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/03] **[On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study](https://arxiv.org/abs/2303.03012)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/ICSE'24-f1b800)
-  [2023/03] **[Stealing the Decoding Algorithms of Language Models](https://arxiv.org/abs/2303.04729)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/CCS'23-f1b800) ![img](https://img.shields.io/badge/Best_paper-ff0000)
-  [2023/02] **[Prompt Stealing Attacks Against Text-to-Image Generation Models](https://arxiv.org/abs/2302.09923)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)

### C5. Membership Inference

-  [2023/12] **[Black-box Membership Inference Attacks against Fine-tuned Diffusion Models](https://arxiv.org/abs/2312.08207)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2023/11] **[Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration](https://arxiv.org/abs/2311.06062)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning](https://arxiv.org/abs/2310.11397)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[User Inference Attacks on Large Language Models](https://arxiv.org/abs/2310.09266)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/09] **[Privacy Side Channels in Machine Learning Systems](https://arxiv.org/abs/2309.05610)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/08] **[White-box Membership Inference Attacks against Diffusion Models](https://arxiv.org/abs/2308.06405)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)
-  [2022/10] **[Membership Inference Attacks Against Text-to-image Generation Models](https://arxiv.org/abs/2210.00968)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)

### C6. Privacy-Preserving Computation

-  [2023/08] **[SIGMA: Secure GPT Inference with Function Secret Sharing](https://eprint.iacr.org/2023/1269)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[CipherGPT: Secure Two-Party GPT Inference](https://eprint.iacr.org/2023/1147)** ![img](https://img.shields.io/badge/LLM-589cf4)

### C7. Property Inference

-  [2023/10] **[Beyond Memorization: Violating Privacy Via Inference with Large Language Models](https://arxiv.org/abs/2310.07298)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[ProPILE: Probing Privacy Leakage in Large Language Models](https://arxiv.org/abs/2307.01881)** ![img](https://img.shields.io/badge/LLM-589cf4) ![img](https://img.shields.io/badge/NeurIPS'23-f1b800)

### C8. Unlearning

-  [2023/10] **[Detecting Pretraining Data from Large Language Models](https://arxiv.org/abs/2310.16789)** <a href="https://swj0419.github.io/detect-pretrain.github.io/"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Large Language Model Unlearning](https://arxiv.org/abs/2310.10683)** <a href="https://github.com/kevinyaobytedance/llm_unlearn"><img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" width="15" height="15"></a> ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[In-Context Unlearning: Language Models as Few Shot Unlearners](https://arxiv.org/abs/2310.07579)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Unlearn What You Want to Forget: Efficient Unlearning for LLMs](https://arxiv.org/abs/2310.20150)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/10] **[Who's Harry Potter? Approximate Unlearning in LLMs](https://arxiv.org/abs/2310.02238?s=08)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/07] **[Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions](https://arxiv.org/abs/2307.03941)** ![img](https://img.shields.io/badge/LLM-589cf4)
-  [2023/03] **[Erasing Concepts from Diffusion Models](https://arxiv.org/abs/2303.07345)** ![img](https://img.shields.io/badge/Diffusion-a99cf4)

# Acknowledgement

- Organizers: [Tianshuo Cong](https://tianshuocong.github.io/), [Xinlei He](https://xinleihe.github.io/), [Zhengyu Zhao](https://zhengyuzhao.github.io/), [Yugeng Liu](https://liu.ai/)

- This project is inspired by [LLM Security](https://llmsecurity.net/), [Awesome LLM Security](https://github.com/corca-ai/awesome-llm-security), [LLM Security & Privacy](https://github.com/chawins/llm-sp),             [UR2-LLMs](https://github.com/jxzhangjhu/Awesome-LLM-Uncertainty-Reliability-Robustness), [PLMpapers](https://github.com/thunlp/PLMpapers), [EvaluationPapers4ChatGPT](https://github.com/THU-KEG/EvaluationPapers4ChatGPT)

<img src="figure/logo.png" alt="image" width="500" height="auto">
