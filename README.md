# LM-SSP
The resources related to the safety, security, and privacy (SSP) of large models (LM).
Here LM contains large language models (LLMs), large vision-language models (LVMs), diffusion models, and so on.

- This repo is in progress ðŸ”¥ (currently manually collect)

- Welcome to recommend resources to us (via Issue/Pull request/[Email](thu_crypto_ai@163.com)/...)!

- Tags: ![img](https://img.shields.io/badge/blog-18a5ab) 
![img](https://img.shields.io/badge/defense-87b800) 
![img](https://img.shields.io/badge/conference-f1b800)
![img](https://img.shields.io/badge/llm-c7688b) 
![img](https://img.shields.io/badge/vlm-589cf4) 
![img](https://img.shields.io/badge/diff-a99cf4)



# Books
- [2024/01] NIST Trustworthy and Responsible AI Reports [[link](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2023.pdf)] 


# Papers

## A.Safety
### A1.Jailbreak
- [2024/01] MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance [[paper](https://arxiv.org/abs/2401.02906)] [[code](https://github.com/pipilurj/MLLM-protector)] ![img](https://img.shields.io/badge/vlm-589cf4)  ![img](https://img.shields.io/badge/defense-87b800) 
- [2023/12] Adversarial Attacks on GPT-4 via Simple Random Search [[paper](https://www.andriushchenko.me/gpt4adv.pdf)] ![img](https://img.shields.io/badge/llm-c7688b) 
- [2023/11] FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts [[paper](https://arxiv.org/abs/2311.05608)] [[code](https://github.com/ThuCCSLab/FigStep)] ![img](https://img.shields.io/badge/vlm-589cf4) 
- [2023/10] Adversarial Attacks on LLMs [[link](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/)] ![img](https://img.shields.io/badge/blog-18a5ab) ![img](https://img.shields.io/badge/llm-c7688b) 
- [2023/07] MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots [[paper](https://arxiv.org/abs/2307.08715)] ![img](https://img.shields.io/badge/llm-c7688b) ![img](https://img.shields.io/badge/NDSS'24-f1b800)
- [2023/07] Jailbroken: How Does LLM Safety Training Fail? [[paper](https://arxiv.org/abs/2307.02483)] ![img](https://img.shields.io/badge/llm-c7688b)

### A2.Safety Alignment
- [2023/12] Exploiting Novel GPT-4 APIs [[paper](https://arxiv.org/abs/2312.14302)] ![img](https://img.shields.io/badge/llm-c7688b) 
- [2023/10] Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To! [[paper](https://arxiv.org/abs/2310.03693)][[code](https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety)] ![img](https://img.shields.io/badge/llm-c7688b) 
- [2023/10] Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models [[paper](https://arxiv.org/abs/2310.02949v1)] [[code](https://github.com/BeyonderXX/ShadowAlignment)] ![img](https://img.shields.io/badge/llm-c7688b) 
- [2023/10] UltraFeedback: Boosting Language Models with High-quality Feedback [[paper](https://arxiv.org/abs/2310.01377)] [[code](https://github.com/OpenBMB/UltraFeedback)] ![img](https://img.shields.io/badge/llm-c7688b) 

### A3.Toxicity
- [2023/08] You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content [[paper](https://arxiv.org/abs/2308.05596)] ![img](https://img.shields.io/badge/llm-c7688b) ![img](https://img.shields.io/badge/S&P'24-f1b800)
- [2023/05] Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models [[paper](https://arxiv.org/abs/2305.13873)] ![img](https://img.shields.io/badge/diff-a99cf4) ![img](https://img.shields.io/badge/CCS'23-f1b800)

### A4.Deepfake

- [2023/03] MGTBench: Benchmarking Machine-Generated Text Detection [[paper](https://arxiv.org/abs/2303.14822)] [[code](https://github.com/xinleihe/MGTBench)] ![img](https://img.shields.io/badge/llm-c7688b) 
- [2022/10] DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Generation Models [[paper](https://arxiv.org/abs/2210.06998)] ![img](https://img.shields.io/badge/diff-a99cf4) ![img](https://img.shields.io/badge/CCS'23-f1b800)

### A5. Agent
- [2023/11] Evil Geniuses: Delving into the Safety of LLM-based Agents [[paper](https://arxiv.org/pdf/2311.11855.pdf)] ![img](https://img.shields.io/badge/llm-c7688b)

## B.Security
### B1.Adversarial Attacks
- [2024/01] INSTRUCTTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models [[paper](https://arxiv.org/pdf/2312.01886.pdf)] ![img](https://img.shields.io/badge/vlm-589cf4) 
- [2023/08] Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models [[paper](https://arxiv.org/abs/2308.07847)] ![img](https://img.shields.io/badge/llm-c7688b) 
- [2023/06] PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts [[paper](https://arxiv.org/abs/2306.04528)] ![img](https://img.shields.io/badge/llm-c7688b) 
- [2022/05] Diffusion Models for Adversarial Purification [[paper](https://arxiv.org/pdf/2205.07460.pdf)] ![img](https://img.shields.io/badge/diff-a99cf4) ![img](https://img.shields.io/badge/defense-87b800) ![img](https://img.shields.io/badge/ICML'22-f1b800) 

### B2.Code Generation
- [2023/02] Large Language Models for Code: Security Hardening and Adversarial Testing [[paper](https://arxiv.org/abs/2302.05319)] ![img](https://img.shields.io/badge/llm-c7688b) ![img](https://img.shields.io/badge/CCS'23-f1b800)
- [2022/11] Do Users Write More Insecure Code with AI Assistants? [[paper](https://arxiv.org/abs/2211.03622)] ![img](https://img.shields.io/badge/llm-c7688b) ![img](https://img.shields.io/badge/CCS'23-f1b800)


### B3.Backdoor Attacks
- Coming soon


### B4.Poisoning Attacks
- Coming soon

## C.Privacy
### C1.Data Reconstruction
- [2023/11] Scalable Extraction of Training Data from (Production) Language Models [[paper](https://arxiv.org/abs/2311.17035)] ![img](https://img.shields.io/badge/llm-c7688b)
- [2023/01] Extracting Training Data from Diffusion Models [[paper](https://www.usenix.org/system/files/usenixsecurity23-carlini.pdf)] ![img](https://img.shields.io/badge/diff-a99cf4) ![img](https://img.shields.io/badge/Security'23-f1b800)
- [2020/12] Extracting Training Data from Large Language Models [[paper](https://arxiv.org/abs/2012.07805)] ![img](https://img.shields.io/badge/llm-c7688b) ![img](https://img.shields.io/badge/Security'21-f1b800)

### C2.Property Inference
- [2023/10] Beyond Memorization: Violating Privacy Via Inference with Large Language Models [[paper](https://arxiv.org/abs/2310.07298)] ![img](https://img.shields.io/badge/llm-c7688b)


### C3.Model Extraction
- [2023/03] Stealing the Decoding Algorithms of Language Models [[paper](https://arxiv.org/abs/2303.04729)] ![img](https://img.shields.io/badge/llm-c7688b) ![img](https://img.shields.io/badge/CCS'23-f1b800)

### C4.Unlearning
- [2023/10] Unlearn What You Want to Forget: Efficient Unlearning for LLMs [[paper](https://arxiv.org/abs/2310.20150)] ![img](https://img.shields.io/badge/llm-c7688b) ![img](https://img.shields.io/badge/defense-87b800) 
- [2023/10] Who's Harry Potter? Approximate Unlearning in LLMs [[paper](https://arxiv.org/abs/2310.02238?s=08)] ![img](https://img.shields.io/badge/llm-c7688b) ![img](https://img.shields.io/badge/defense-87b800) 
- [2023/03] Erasing Concepts from Diffusion Models [[paper](https://arxiv.org/abs/2303.07345)] ![img](https://img.shields.io/badge/diff-a99cf4) ![img](https://img.shields.io/badge/defense-87b800) 

### C5.Copyright
- [2024/01] Generative AI Has a Visual Plagiarism Problem [[link](https://spectrum.ieee.org/midjourney-copyright)] ![img](https://img.shields.io/badge/blog-18a5ab)
- [2023/11] Protecting Intellectual Property of Large Language Model-Based Code Generation APIs via Watermarks [[paper](https://dl.acm.org/doi/abs/10.1145/3576915.3623120)] ![img](https://img.shields.io/badge/llm-c7688b) ![img](https://img.shields.io/badge/CCS'23-f1b800) ![img](https://img.shields.io/badge/defense-87b800)
